11-17-17

Downloading data from Tony's account using wget

downloaded supernova as per website description and ran sitecheck and testrun

11-21-17

Downloaded trimmomatic-0.36 in the program subfolder
Starting to trim the original raw reads

11-22-17

Finished trimming - statistics can be found in /scripts/trim/phgenome_trim.output

Started fastqc (version 0.11) to look at the quality of output

FastQC looks fine, but files are unrecognizeable by supernova after trimming. Thus, I will assemble using raw reads.

Started assembly with supernova (script in /scirpts/assembly/ directory)

11-29-17

Starting run of the assembly on Mike Miller's bigmem node

srun -A millermrgrp -p bigmemh -t 5 hostname

2-12-18

Downloaded phgenome3 (assembly with 56x coverage) of Pacific herring genome
Used mkoutput to create a fasta file with bubble information in it (megabubble). Found in scripts/fasta/

2-27-18

converted .gz to .bgz with bgzip
indexing genome to find true size of assembly including smaller scaffolds

3-30-18

Subsetted the genome using cat of the number of reads we need divided by 2 for each of the R1 and R2.
Number of reads calculated by coverage for the following:
55x - 340 000 000
70x - 433 000 000
85x - 526 000 000
100x - 618 000 000
128x - 771 607 516

Then I concatenate the resulting files and run custom python script on them to determine # of unique kmers

4-20-18

used anaconda3 module: module load anaconda3

created an environment in phgenome:
conda create -n phgenome python=3.6

should activate with: source activate phgenome

Need to install pip in the environment (otherwise it uses global pip)

conda install -n phgenome pip

6-18-18
--------
trying long ranger basic for filtering and creating a bam file out of the fastqs
